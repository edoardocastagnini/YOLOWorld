# YOLOWorld
üß† YOLOWorld ‚Äì PhiNet Extension with Distillation & Pruning

This repository extends the original Ultralytics YOLOWorld framework to explore model compression techniques ‚Äî specifically Knowledge Distillation and Structured Pruning ‚Äî applied to a custom YOLOWorld architecture with a PhiNet backbone.

‚∏ª

üîß Overview

The project is divided into two main modules:
	1.	Distillation ‚Äì integrates a new Distillation Loss into the YOLO training pipeline.
	2.	Pruning ‚Äì implements both standard and custom pruning methods for YOLOWorld components.

Both modules build directly on top of the Ultralytics YOLOWorld codebase, keeping the same structure, with a few added or modified files.

‚∏ª

üß© Custom YOLOWorld Architecture

The base model used here is a custom YOLOWorld where the original backbone has been replaced with a PhiNet block.
This version maintains the standard YOLO head and decoder, ensuring full compatibility with the YOLO training and evaluation routines.

‚∏ª

üî¨ 1. Knowledge Distillation

A new Distillation Loss was implemented inside the YOLO training loop, combined with the three standard YOLO losses:
	‚Ä¢	Box loss
	‚Ä¢	Class loss
	‚Ä¢	Distribution focal loss

The distillation loss term encourages the student model to mimic intermediate representations and predictions of the teacher model.

üß™ Experiments

Several experiments were carried out:
	‚Ä¢	Varying the weight of the standard YOLO losses (original, halved, quartered).
	‚Ä¢	Testing different strengths of the distillation term.
	‚Ä¢	Distilling:
	‚Ä¢	YOLOWorld-N (student) from YOLOWorld-N (teacher)
	‚Ä¢	YOLOWorld-N (student) from YOLOWorld-S (teacher)
	‚Ä¢	YOLOWorld-S (student) from YOLOWorld-S (teacher)

The goal was to find the optimal trade-off between accuracy, stability, and student compactness.

‚∏ª

‚úÇÔ∏è 2. Pruning

The pruning stage focused on reducing model size and complexity while maintaining detection performance.

Two distinct approaches were used:
	‚Ä¢	Backbone + Decoder ‚Üí standard Torch-Pruning workflow.
	‚Ä¢	Transformer Head (C2fAttention) ‚Üí custom pruning method developed for this project.

This was necessary because the C2fAttention layers in YOLOWorld are not natively compatible with Torch-Pruning dependency tracing.
A custom implementation handles zeroing and structural pruning safely within the transformer head.

üß† Importance Criteria

Three importance metrics were tested:
	‚Ä¢	Magnitude ‚Äì classical weight-based criterion.
	‚Ä¢	Taylor ‚Äì uses first-order Taylor expansion to estimate loss sensitivity.
	‚Ä¢	LAMP ‚Äì Layer-Adaptive Magnitude Pruning, balancing pruning across layers.

Each combination of backbone/decoder and transformer head prune rates was tested to study performance trade-offs.

‚∏ª
