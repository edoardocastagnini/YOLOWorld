# YOLOWorld â€“ PhiNet Extension with Distillation & Pruning

This repository extends the original [Ultralytics YOLOWorld](https://github.com/ultralytics/ultralytics) framework to explore **model compression techniques** â€” specifically **Knowledge Distillation** and **Pruning** â€” applied to a **custom YOLOWorld architecture** with a **PhiNet backbone**.

---

## ğŸ”§ Overview

The project is divided into two main modules:

1. **Distillation** â€“ integrates a new *Distillation Loss* into the YOLO training pipeline.  
2. **Pruning** â€“ implements both standard and custom pruning methods for YOLOWorld components.

Both modules build directly on top of the **Ultralytics YOLOWorld codebase**, keeping the same structure, with a few added or modified files.

---

## ğŸ§© Custom YOLOWorld Architecture

The base model used here is a **custom YOLOWorld** where the **original backbone** has been replaced with a **PhiNet** block.  
This version maintains the standard YOLO head and decoder, ensuring full compatibility with the YOLO training and evaluation routines.

---

## ğŸ”¬ 1. Knowledge Distillation

A new **Distillation Loss** was implemented inside the YOLO training loop, combined with the three standard YOLO losses:

- **Box loss**  
- **Class loss**  
- **Distribution focal loss**  

The *distillation loss* term encourages the student model to mimic intermediate representations and predictions of the teacher model.

### ğŸ§ª Experiments

Several experiments were carried out:

- Varying the weight of the standard YOLO losses (original, halved, quartered).  
- Testing different strengths of the distillation term.  
- Distilling:  
  - **YOLOWorld-N (student)** from **YOLOWorld-N (teacher)**
  - **YOLOWorld-N (student)** from **YOLOWorld-S (teacher)**  
  - **YOLOWorld-S (student)** from **YOLOWorld-S (teacher)**  

The goal was to find the optimal trade-off between **accuracy**, **stability**, and **student compactness**.

---

## âœ‚ï¸ 2. Pruning

The pruning stage focused on **reducing model size and complexity** while maintaining detection performance.

Two distinct approaches were used:

- **Backbone + Decoder** â†’ standard *Torch-Pruning* workflow.  
- **Transformer Head (C2fAttn modules)** â†’ custom pruning method developed for this project.

This was necessary because the **C2fAttn layers** in YOLOWorld are **not natively compatible** with Torch-Pruning dependency tracing.  
A custom implementation handles zeroing and structural pruning safely within the transformer head.

### ğŸ§  Importance Criteria

Three importance metrics were tested:

- **Magnitude** â€“ classical weight-based criterion.  
- **Taylor** â€“ uses first-order Taylor expansion to estimate loss sensitivity.  
- **LAMP** â€“ Layer-Adaptive Magnitude Pruning, balancing pruning across layers.  

Each combination of backbone/decoder and transformer head prune rates was tested to study performance trade-offs.

---

## ğŸ“ Repository Structure

â”œâ”€â”€ distillation/                          # Knowledge distillation experiments
â”‚   â”œâ”€â”€ datasets/                          # Dataset directory
â”‚   â”‚   â””â”€â”€ README.md                      # Instructions for dataset download and structure
â”‚   â”‚
â”‚   â”œâ”€â”€ ultralytics/                       # Modified Ultralytics YOLO base code
â”‚   â”‚   â”œâ”€â”€ cfg/                           # Config files (includes distillation.yaml and PhiNet YAML)
â”‚   â”‚   â”‚   â””â”€â”€ models/v8/yolov8-worldv2-phinet.yaml
â”‚   â”‚   â”œâ”€â”€ data/                          # Dataset configs (e.g. coco.yaml)
â”‚   â”‚   â”œâ”€â”€ distillation/                  # Custom distillation loss and utilities
â”‚   â”‚   â”œâ”€â”€ engine/                        # Training engine and DDP setup
â”‚   â”‚   â”œâ”€â”€ hub/                           # Model hub and export tools
â”‚   â”‚   â”œâ”€â”€ models/                        # YOLO model definitions
â”‚   â”‚   â”œâ”€â”€ nn/                            # Neural network modules (includes PhiNet backbone)
â”‚   â”‚   â”œâ”€â”€ solutions/                     # Optional YOLO utilities
â”‚   â”‚   â”œâ”€â”€ trackers/                      # Object tracking components
â”‚   â”‚   â”œâ”€â”€ utils/                         # Helper functions
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ distill.py                         # Main training script for knowledge distillation
â”‚   â”œâ”€â”€ requirements.txt                   # Dependencies for distillation
â”‚   â”œâ”€â”€ train_args.yaml                    # Default distillation training args
â”‚   â”œâ”€â”€ ywv2-n-trained100epochs.pt         # Teacher model (YOLO-WorldV2-N)
â”‚   â”œâ”€â”€ ywv2-s-trained100epochs.pt         # Teacher model (YOLO-WorldV2-S)
â”‚   â””â”€â”€ README.md                          # Distillation instructions
â”‚
â”œâ”€â”€ pruning/                               # Model pruning experiments
â”‚   â”œâ”€â”€ datasets/                          # Dataset directory
â”‚   â”‚   â””â”€â”€ README.md                      # Instructions for dataset download and structure
â”‚   â”‚
â”‚   â”œâ”€â”€ ultralytics/                       # Modified Ultralytics YOLO base code
â”‚   â”‚   â”œâ”€â”€ cfg/                           # Config files
â”‚   â”‚   â”œâ”€â”€ data/                          # Dataset configs
â”‚   â”‚   â”œâ”€â”€ engine/                        # Training engine
â”‚   â”‚   â”œâ”€â”€ hub/                           # Model hub and export tools
â”‚   â”‚   â”œâ”€â”€ models/                        # YOLO model definitions
â”‚   â”‚   â”œâ”€â”€ nn/                            # Neural network modules (PhiNet backbone)
â”‚   â”‚   â”œâ”€â”€ solutions/                     # Optional YOLO utilities
â”‚   â”‚   â”œâ”€â”€ trackers/                      # Object tracking components
â”‚   â”‚   â”œâ”€â”€ utils/                         # Helper functions
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ywv2_pruning_magn.py               # Magnitude-based pruning
â”‚   â”œâ”€â”€ ywv2_pruning_taylor.py             # Taylor-based pruning
â”‚   â”œâ”€â”€ ywv2_pruning_lamp.py               # LAMP-based pruning
â”‚   â”œâ”€â”€ requirements.txt                   # Dependencies for pruning
â”‚   â”œâ”€â”€ train_args.yaml                    # Training configuration after pruning
â”‚   â”œâ”€â”€ ywv2-n-trained100epochs.pt         # Pretrained model (YOLO-WorldV2-N)
â”‚   â”œâ”€â”€ ywv2-s-trained100epochs.pt         # Pretrained model (YOLO-WorldV2-S)
â”‚   â””â”€â”€ README.md                          # Pruning instructions
â”‚
â””â”€â”€ README.md                              # Main repository documentation
